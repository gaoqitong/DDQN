{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from env_current import *\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed = 123):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "        \n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "    exploration_rate = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Exploration\", exploration_rate)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, exploration_rate]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNet(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, batch_size, save_path):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.inputs, self.q_values, self.a_predict = self.build_net()\n",
    "        self.net_params = tf.trainable_variables()\n",
    "        \n",
    "        self.target_inputs, self.target_q_values, self.target_a_predict = self.build_net()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params):]\n",
    "        \n",
    "        self.update_target_net_params = [self.target_net_params[i]\n",
    "                                         .assign(tf.multiply(self.tau, self.net_params[i])\n",
    "                                                 + tf.multiply((1.-self.tau), self.target_net_params[i]) ) \n",
    "                                         for i in range(len(self.target_net_params))]\n",
    "        \n",
    "        self.true_q_value = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "        \n",
    "        gather_indices = tf.range(MINIBATCH_SIZE) * tf.shape(self.q_values)[1] + tf.reshape(self.action, [-1])\n",
    "        self.action_correlated_q = tf.gather(tf.reshape(self.q_values,[-1]), gather_indices)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(tf.reshape(self.true_q_value, [-1]), self.action_correlated_q)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.last_num_epi = -1\n",
    "        \n",
    "    def build_net(self):\n",
    "        s_inputs = tf.placeholder(shape = [None, self.state_dim], dtype = tf.float32)\n",
    "        W1 = tf.Variable(tf.random_uniform([self.state_dim, 400], 0, 0.1))\n",
    "        B1 = tf.Variable(tf.zeros([400]))\n",
    "        L1 = tf.add(tf.matmul(s_inputs, W1), B1)\n",
    "        L1 = tf.layers.batch_normalization(L1)\n",
    "        L1 = tf.nn.relu(L1)\n",
    "        W2 = tf.Variable(tf.random_uniform([400, 300], 0, 0.1))\n",
    "        B2 = tf.Variable(tf.zeros([300]))\n",
    "        L2 = tf.add(tf.matmul(L1, W2), B2)\n",
    "        L2 = tf.layers.batch_normalization(L2)\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        W3 = tf.Variable(tf.random_uniform([300, self.action_dim], 0, 0.01))\n",
    "#         B3 = tf.Variable(tf.random_uniform([self.action_dim], -0.003, 0.003))\n",
    "#         q_values = tf.add(tf.matmul(L2, W3), B3)\n",
    "        q_values = tf.matmul(L2, W3)  \n",
    "        a_predict = tf.argmax(q_values,1)\n",
    "        \n",
    "        regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "        tf.contrib.layers.apply_regularization(regularizer,[W1, B1, W2, B2, W3])\n",
    "        return s_inputs, q_values, a_predict\n",
    "    \n",
    "    def train(self, states, action, true_q, num_epi):\n",
    "        if num_epi%20 == 0 and num_epi!=self.last_num_epi:\n",
    "            self.saver.save(self.sess, self.save_path)\n",
    "            print \"DDQN Saved\"\n",
    "            self.last_num_epi = num_epi\n",
    "            \n",
    "        return self.sess.run([self.q_values, self.optimizer], \n",
    "                             feed_dict={self.inputs: states, self.true_q_value: true_q, self.action: action})\n",
    "    \n",
    "    def predict_q(self, states):\n",
    "        return self.sess.run(self.q_values, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predict_a(self, states):\n",
    "        return self.sess.run(self.a_predict, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predect_target(self, states):\n",
    "        return self.sess.run(self.target_q_values, feed_dict={self.target_inputs: states})\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.sess.run(self.update_target_net_params)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, env, qnet):\n",
    "    \n",
    "    global EXPLORATION_RATE\n",
    "  \n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    \n",
    "    qnet.update_target()\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    for num_epi in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "        s = [list(np.unravel_index(s, env.shape))]\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "\n",
    "            a = np.argmax(qnet.predict_q(np.reshape(s, (1, qnet.state_dim))))\n",
    "    \n",
    "            if np.random.rand(1) < EXPLORATION_RATE:\n",
    "                s2, r, terminal, info = env.step(np.random.randint(0,qnet.action_dim))\n",
    "            else:\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "            \n",
    "            s2 = list(np.unravel_index(s2, env.shape))\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (qnet.state_dim,)), np.reshape(a, (1,)), r,\n",
    "                              terminal, np.reshape(s2, (qnet.state_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = qnet.predect_target(s2_batch)\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * np.amax(target_q[k]))\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = qnet.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)), num_epi)\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "                # Update target networks\n",
    "                qnet.update_target()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j == MAX_EPISODE_LEN-1:\n",
    "                \n",
    "                if EXPLORATION_RATE > 0.05 and terminal:\n",
    "                    EXPLORATION_RATE = EXPLORATION_RATE*0.92\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j),\n",
    "                    summary_vars[2]: EXPLORATION_RATE\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, num_epi)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} | Exploration: {:.6f} '.format(int(ep_reward), \\\n",
    "                        num_epi, (ep_ave_max_q / float(j)), EXPLORATION_RATE))\n",
    "                \n",
    "                f = open(\"stats.txt\", \"ab\")\n",
    "                f.write(\"| Reward: \" + str(int(ep_reward)) \n",
    "                        +\" | Episode: \" + str(num_epi) \n",
    "                        + \" | Qmax: \" + str(ep_ave_max_q / float(j)) \n",
    "                        + \" | Exploration: \" + str(EXPLORATION_RATE) + \"\\n\")\n",
    "                f.close()\n",
    "                \n",
    "                break\n",
    "                \n",
    "        if num_epi%1 == 0:\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            world = np.zeros(env.shape)\n",
    "            for state in range(env.nS):\n",
    "                state = np.unravel_index(state, env.shape)\n",
    "                action = qnet.predict_q(np.reshape(state, (1,state_dim)))\n",
    "                action = np.argmax(action)\n",
    "                state_list.append(state)\n",
    "                action_list.append(action)\n",
    "                \n",
    "#             print np.reshape(action_list, env.shape)\n",
    "                \n",
    "            f = open(\"action.txt\",\"ab\")\n",
    "            np.savetxt(f, np.reshape(action_list, env.shape), fmt=\"%i\")\n",
    "            f.write(\"---------------------------\\n\")\n",
    "            f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0015\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 10**6\n",
    "MINIBATCH_SIZE = 64\n",
    "RANDOM_SEED = 272\n",
    "MAX_EPISODES = 50000\n",
    "MAX_EPISODE_LEN = 500\n",
    "SUMMARY_DIR = './results/tf_ddqn'\n",
    "EXPLORATION_RATE = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 0 | Qmax: 84.7947 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 1 | Qmax: 143.6896 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 2 | Qmax: 159.7251 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 3 | Qmax: 172.0265 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 4 | Qmax: 173.8788 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 5 | Qmax: 173.8859 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 6 | Qmax: 174.0377 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 7 | Qmax: 172.8736 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 8 | Qmax: 171.6056 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 9 | Qmax: 170.7364 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 10 | Qmax: 169.2289 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 11 | Qmax: 167.6618 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 12 | Qmax: 166.0959 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 13 | Qmax: 164.5093 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 14 | Qmax: 163.0243 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 15 | Qmax: 161.4573 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 16 | Qmax: 159.7517 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 17 | Qmax: 157.9788 | Exploration: 0.650000 \n",
      "| Reward: -109 | Episode: 18 | Qmax: 158.2855 | Exploration: 0.598000 \n",
      "| Reward: -500 | Episode: 19 | Qmax: 155.8903 | Exploration: 0.598000 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 20 | Qmax: 154.2040 | Exploration: 0.598000 \n",
      "| Reward: -500 | Episode: 21 | Qmax: 152.4479 | Exploration: 0.598000 \n",
      "| Reward: -500 | Episode: 22 | Qmax: 150.9510 | Exploration: 0.598000 \n",
      "| Reward: -57 | Episode: 23 | Qmax: 152.3264 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 24 | Qmax: 149.2167 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 25 | Qmax: 147.6241 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 26 | Qmax: 146.1554 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 27 | Qmax: 144.4823 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 28 | Qmax: 143.0259 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 29 | Qmax: 141.5610 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 30 | Qmax: 139.9983 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 31 | Qmax: 138.6763 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 32 | Qmax: 137.2153 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 33 | Qmax: 135.8457 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 34 | Qmax: 134.4359 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 35 | Qmax: 133.1271 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 36 | Qmax: 131.8823 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 37 | Qmax: 130.6123 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 38 | Qmax: 129.3395 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 39 | Qmax: 128.1484 | Exploration: 0.550160 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 40 | Qmax: 126.7737 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 41 | Qmax: 125.5468 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 42 | Qmax: 124.3112 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 43 | Qmax: 123.0122 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 44 | Qmax: 121.7058 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 45 | Qmax: 120.4197 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 46 | Qmax: 119.2222 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 47 | Qmax: 118.0245 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 48 | Qmax: 116.7965 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 49 | Qmax: 115.6847 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 50 | Qmax: 114.4528 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 51 | Qmax: 113.3495 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 52 | Qmax: 112.2043 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 53 | Qmax: 111.1068 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 54 | Qmax: 109.9300 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 55 | Qmax: 108.7050 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 56 | Qmax: 107.5377 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 57 | Qmax: 106.4648 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 58 | Qmax: 105.3977 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 59 | Qmax: 104.2383 | Exploration: 0.550160 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 60 | Qmax: 103.1064 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 61 | Qmax: 102.0335 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 62 | Qmax: 100.9933 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 63 | Qmax: 99.8786 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 64 | Qmax: 98.8243 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 65 | Qmax: 97.7742 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 66 | Qmax: 96.7298 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 67 | Qmax: 95.6425 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 68 | Qmax: 94.6444 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 69 | Qmax: 93.6243 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 70 | Qmax: 92.6224 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 71 | Qmax: 91.6159 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 72 | Qmax: 90.6334 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 73 | Qmax: 89.6465 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 74 | Qmax: 88.6834 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 75 | Qmax: 87.6831 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 76 | Qmax: 86.7273 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 77 | Qmax: 85.7759 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 78 | Qmax: 84.8142 | Exploration: 0.550160 \n",
      "| Reward: -500 | Episode: 79 | Qmax: 83.8865 | Exploration: 0.550160 \n",
      "DDQN Saved\n",
      "| Reward: -73 | Episode: 80 | Qmax: 84.3656 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 81 | Qmax: 82.7732 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 82 | Qmax: 81.8397 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 83 | Qmax: 80.9271 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 84 | Qmax: 80.0124 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 85 | Qmax: 79.0459 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 86 | Qmax: 78.1069 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 87 | Qmax: 77.1723 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 88 | Qmax: 76.2694 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 89 | Qmax: 75.3435 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 90 | Qmax: 74.4419 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 91 | Qmax: 73.5166 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 92 | Qmax: 72.6310 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 93 | Qmax: 71.7330 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 94 | Qmax: 70.8289 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 95 | Qmax: 69.9292 | Exploration: 0.506147 \n",
      "| Reward: -500 | Episode: 96 | Qmax: 69.0391 | Exploration: 0.506147 \n",
      "| Reward: -77 | Episode: 97 | Qmax: 69.2973 | Exploration: 0.465655 \n",
      "| Reward: -69 | Episode: 98 | Qmax: 69.3252 | Exploration: 0.428403 \n",
      "| Reward: -500 | Episode: 99 | Qmax: 67.8755 | Exploration: 0.428403 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 100 | Qmax: 67.0337 | Exploration: 0.428403 \n",
      "| Reward: -500 | Episode: 101 | Qmax: 66.1688 | Exploration: 0.428403 \n",
      "| Reward: -500 | Episode: 102 | Qmax: 65.3169 | Exploration: 0.428403 \n",
      "| Reward: -500 | Episode: 103 | Qmax: 64.4528 | Exploration: 0.428403 \n",
      "| Reward: -73 | Episode: 104 | Qmax: 64.6709 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 105 | Qmax: 63.4571 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 106 | Qmax: 62.6063 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 107 | Qmax: 61.7541 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 108 | Qmax: 60.8874 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 109 | Qmax: 60.0520 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 110 | Qmax: 59.2397 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 111 | Qmax: 58.3728 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 112 | Qmax: 57.5357 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 113 | Qmax: 56.7145 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 114 | Qmax: 55.9199 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 115 | Qmax: 55.1323 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 116 | Qmax: 54.3146 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 117 | Qmax: 53.5179 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 118 | Qmax: 52.7044 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 119 | Qmax: 51.9164 | Exploration: 0.394131 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 120 | Qmax: 51.1220 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 121 | Qmax: 50.3048 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 122 | Qmax: 49.5496 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 123 | Qmax: 48.7528 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 124 | Qmax: 47.9710 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 125 | Qmax: 47.2042 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 126 | Qmax: 46.4304 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 127 | Qmax: 45.6715 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 128 | Qmax: 44.9061 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 129 | Qmax: 44.1519 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 130 | Qmax: 43.3976 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 131 | Qmax: 42.6384 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 132 | Qmax: 41.9002 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 133 | Qmax: 41.1452 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 134 | Qmax: 40.4084 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 135 | Qmax: 39.6783 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 136 | Qmax: 38.9337 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 137 | Qmax: 38.2275 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 138 | Qmax: 37.5083 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 139 | Qmax: 36.8125 | Exploration: 0.394131 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 140 | Qmax: 36.1053 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 141 | Qmax: 35.3917 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 142 | Qmax: 34.7020 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 143 | Qmax: 34.0025 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 144 | Qmax: 33.3082 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 145 | Qmax: 32.6171 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 146 | Qmax: 31.9243 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 147 | Qmax: 31.2386 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 148 | Qmax: 30.5602 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 149 | Qmax: 29.8775 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 150 | Qmax: 29.1937 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 151 | Qmax: 28.5278 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 152 | Qmax: 27.8425 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 153 | Qmax: 27.1767 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 154 | Qmax: 26.5053 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 155 | Qmax: 25.8453 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 156 | Qmax: 25.1883 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 157 | Qmax: 24.5413 | Exploration: 0.394131 \n",
      "| Reward: -500 | Episode: 158 | Qmax: 23.8920 | Exploration: 0.394131 \n",
      "| Reward: -55 | Episode: 159 | Qmax: 23.9260 | Exploration: 0.362600 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 160 | Qmax: 23.1813 | Exploration: 0.362600 \n",
      "| Reward: -49 | Episode: 161 | Qmax: 23.2558 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 162 | Qmax: 22.4593 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 163 | Qmax: 21.8236 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 164 | Qmax: 21.1882 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 165 | Qmax: 20.5580 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 166 | Qmax: 19.9077 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 167 | Qmax: 19.2886 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 168 | Qmax: 18.6620 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 169 | Qmax: 18.0342 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 170 | Qmax: 17.4118 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 171 | Qmax: 16.8012 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 172 | Qmax: 16.1887 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 173 | Qmax: 15.5807 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 174 | Qmax: 14.9727 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 175 | Qmax: 14.3914 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 176 | Qmax: 13.7919 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 177 | Qmax: 13.2000 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 178 | Qmax: 12.6089 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 179 | Qmax: 12.0233 | Exploration: 0.333592 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 180 | Qmax: 11.4302 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 181 | Qmax: 10.8454 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 182 | Qmax: 10.2685 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 183 | Qmax: 9.6892 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 184 | Qmax: 9.1098 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 185 | Qmax: 8.5318 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 186 | Qmax: 7.9536 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 187 | Qmax: 7.3809 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 188 | Qmax: 6.8033 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 189 | Qmax: 6.2320 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 190 | Qmax: 5.6568 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 191 | Qmax: 5.0837 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 192 | Qmax: 4.5055 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 193 | Qmax: 3.9230 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 194 | Qmax: 3.3397 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 195 | Qmax: 2.7565 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 196 | Qmax: 2.1716 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 197 | Qmax: 1.5834 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 198 | Qmax: 0.9921 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 199 | Qmax: 0.3997 | Exploration: 0.333592 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 200 | Qmax: -0.1923 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 201 | Qmax: -0.7391 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 202 | Qmax: -1.2279 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 203 | Qmax: -1.6873 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 204 | Qmax: -2.1243 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 205 | Qmax: -2.5490 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 206 | Qmax: -2.9780 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 207 | Qmax: -3.4014 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 208 | Qmax: -3.8268 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 209 | Qmax: -4.2686 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 210 | Qmax: -4.7201 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 211 | Qmax: -5.1643 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 212 | Qmax: -5.6357 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 213 | Qmax: -6.1014 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 214 | Qmax: -6.5494 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 215 | Qmax: -6.9777 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 216 | Qmax: -7.4098 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 217 | Qmax: -7.8730 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 218 | Qmax: -8.3029 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 219 | Qmax: -8.6980 | Exploration: 0.333592 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 220 | Qmax: -9.1289 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 221 | Qmax: -9.5532 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 222 | Qmax: -9.9506 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 223 | Qmax: -10.3620 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 224 | Qmax: -10.7840 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 225 | Qmax: -11.2099 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 226 | Qmax: -11.6284 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 227 | Qmax: -12.0427 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 228 | Qmax: -12.4534 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 229 | Qmax: -12.8489 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 230 | Qmax: -13.2600 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 231 | Qmax: -13.6679 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 232 | Qmax: -14.0869 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 233 | Qmax: -14.4937 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 234 | Qmax: -14.8934 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 235 | Qmax: -15.2835 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 236 | Qmax: -15.6959 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 237 | Qmax: -16.1029 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 238 | Qmax: -16.5045 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 239 | Qmax: -16.9070 | Exploration: 0.333592 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 240 | Qmax: -17.3104 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 241 | Qmax: -17.6999 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 242 | Qmax: -18.0702 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 243 | Qmax: -18.4663 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 244 | Qmax: -18.8579 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 245 | Qmax: -19.2608 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 246 | Qmax: -19.6501 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 247 | Qmax: -20.0318 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 248 | Qmax: -20.4198 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 249 | Qmax: -20.8077 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 250 | Qmax: -21.2042 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 251 | Qmax: -21.6035 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 252 | Qmax: -21.9878 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 253 | Qmax: -22.3602 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 254 | Qmax: -22.7251 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 255 | Qmax: -23.1109 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 256 | Qmax: -23.4845 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 257 | Qmax: -23.8620 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 258 | Qmax: -24.2282 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 259 | Qmax: -24.6096 | Exploration: 0.333592 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 260 | Qmax: -24.9726 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 261 | Qmax: -25.3453 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 262 | Qmax: -25.7075 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 263 | Qmax: -26.0639 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 264 | Qmax: -26.4352 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 265 | Qmax: -26.7951 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 266 | Qmax: -27.1255 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 267 | Qmax: -27.4426 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 268 | Qmax: -27.8000 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 269 | Qmax: -28.0943 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 270 | Qmax: -28.4765 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 271 | Qmax: -28.8335 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 272 | Qmax: -29.1118 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 273 | Qmax: -29.4584 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 274 | Qmax: -29.6869 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 275 | Qmax: -29.9752 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 276 | Qmax: -30.3672 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 277 | Qmax: -30.6257 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 278 | Qmax: -30.8995 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 279 | Qmax: -31.2524 | Exploration: 0.333592 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 280 | Qmax: -31.5734 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 281 | Qmax: -31.9087 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 282 | Qmax: -32.1595 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 283 | Qmax: -32.4552 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 284 | Qmax: -32.8122 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 285 | Qmax: -33.1013 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 286 | Qmax: -33.3834 | Exploration: 0.333592 \n",
      "| Reward: -500 | Episode: 287 | Qmax: -33.7148 | Exploration: 0.333592 \n",
      "| Reward: -55 | Episode: 288 | Qmax: -34.3112 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 289 | Qmax: -34.0314 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 290 | Qmax: -34.2772 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 291 | Qmax: -34.5650 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 292 | Qmax: -34.8754 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 293 | Qmax: -35.1289 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 294 | Qmax: -35.4683 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 295 | Qmax: -35.7029 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 296 | Qmax: -35.9201 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 297 | Qmax: -36.0623 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 298 | Qmax: -36.4594 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 299 | Qmax: -36.7538 | Exploration: 0.306905 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 300 | Qmax: -37.1615 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 301 | Qmax: -37.2755 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 302 | Qmax: -37.5893 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 303 | Qmax: -37.7905 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 304 | Qmax: -37.9555 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 305 | Qmax: -38.4707 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 306 | Qmax: -38.6935 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 307 | Qmax: -38.8697 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 308 | Qmax: -39.1332 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 309 | Qmax: -39.3765 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 310 | Qmax: -39.6099 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 311 | Qmax: -39.7180 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 312 | Qmax: -40.1132 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 313 | Qmax: -40.2621 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 314 | Qmax: -40.6561 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 315 | Qmax: -40.8995 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 316 | Qmax: -41.0727 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 317 | Qmax: -41.4093 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 318 | Qmax: -41.7895 | Exploration: 0.306905 \n",
      "| Reward: -500 | Episode: 319 | Qmax: -41.8152 | Exploration: 0.306905 \n",
      "DDQN Saved\n",
      "| Reward: -93 | Episode: 320 | Qmax: -42.2952 | Exploration: 0.282352 \n",
      "| Reward: -61 | Episode: 321 | Qmax: -42.7262 | Exploration: 0.259764 \n",
      "| Reward: -45 | Episode: 322 | Qmax: -42.9762 | Exploration: 0.238983 \n",
      "| Reward: -500 | Episode: 323 | Qmax: -42.1666 | Exploration: 0.238983 \n",
      "| Reward: -41 | Episode: 324 | Qmax: -43.3889 | Exploration: 0.219864 \n",
      "| Reward: -43 | Episode: 325 | Qmax: -43.7411 | Exploration: 0.202275 \n",
      "| Reward: -51 | Episode: 326 | Qmax: -43.0599 | Exploration: 0.186093 \n",
      "| Reward: -500 | Episode: 327 | Qmax: -42.4563 | Exploration: 0.186093 \n",
      "| Reward: -41 | Episode: 328 | Qmax: -43.9498 | Exploration: 0.171206 \n",
      "| Reward: -47 | Episode: 329 | Qmax: -43.5642 | Exploration: 0.157509 \n",
      "| Reward: -37 | Episode: 6957 | Qmax: -1.0432 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6958 | Qmax: -0.7814 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6959 | Qmax: -0.8636 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 6960 | Qmax: -0.9741 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6961 | Qmax: -0.9546 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 6962 | Qmax: -1.1152 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 6963 | Qmax: -0.7381 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6964 | Qmax: -0.7931 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6965 | Qmax: -0.7797 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6966 | Qmax: -0.7402 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6967 | Qmax: -0.8508 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6968 | Qmax: -0.5281 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6969 | Qmax: -0.9289 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6970 | Qmax: -0.8285 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6971 | Qmax: -0.9961 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 6972 | Qmax: -0.9612 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 6973 | Qmax: -0.6288 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6974 | Qmax: -0.9088 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6975 | Qmax: -0.6178 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6976 | Qmax: -0.8146 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6977 | Qmax: -0.9031 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6978 | Qmax: -1.1587 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6979 | Qmax: -0.9353 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 6980 | Qmax: -1.2864 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6981 | Qmax: -0.9407 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6982 | Qmax: -1.0268 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6983 | Qmax: -0.8759 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 6984 | Qmax: -0.8552 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 6985 | Qmax: -0.6838 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6986 | Qmax: -0.7630 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6987 | Qmax: -0.5696 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6988 | Qmax: -0.5857 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6989 | Qmax: -0.8133 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6990 | Qmax: -0.6835 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6991 | Qmax: -0.8144 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6992 | Qmax: -0.8611 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 6993 | Qmax: -1.0176 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6994 | Qmax: -0.6450 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6995 | Qmax: -0.8125 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6996 | Qmax: -0.4621 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 6997 | Qmax: -0.9134 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6998 | Qmax: -0.9123 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 6999 | Qmax: -0.8104 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 7000 | Qmax: -0.7336 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7001 | Qmax: -0.9120 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7002 | Qmax: -0.9604 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7003 | Qmax: -0.8185 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7004 | Qmax: -0.8148 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7005 | Qmax: -0.7169 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7006 | Qmax: -1.0978 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7007 | Qmax: -0.7897 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7008 | Qmax: -1.0816 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7009 | Qmax: -0.8637 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7010 | Qmax: -0.9664 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7011 | Qmax: -0.8443 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7012 | Qmax: -1.1069 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7013 | Qmax: -1.1635 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7014 | Qmax: -0.8726 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7015 | Qmax: -0.8832 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7016 | Qmax: -0.8206 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7017 | Qmax: -0.7484 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7018 | Qmax: -0.8678 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7019 | Qmax: -0.9018 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7020 | Qmax: -0.7012 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7021 | Qmax: -1.1309 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7022 | Qmax: -0.8331 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7023 | Qmax: -0.8187 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7024 | Qmax: -0.8231 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7025 | Qmax: -0.9349 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7026 | Qmax: -0.8616 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7027 | Qmax: -0.9214 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7028 | Qmax: -0.9247 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7029 | Qmax: -0.8688 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7030 | Qmax: -0.6877 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7031 | Qmax: -0.6711 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7032 | Qmax: -0.9716 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7033 | Qmax: -1.0751 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7034 | Qmax: -0.8395 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7035 | Qmax: -0.8452 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7036 | Qmax: -1.1107 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7037 | Qmax: -0.9453 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7038 | Qmax: -1.0645 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7039 | Qmax: -0.8337 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 7040 | Qmax: -1.1883 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7041 | Qmax: -0.8911 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7042 | Qmax: -1.2244 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7043 | Qmax: -0.7430 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7044 | Qmax: -1.1587 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7045 | Qmax: -1.0496 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7046 | Qmax: -0.8269 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7047 | Qmax: -0.9059 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7048 | Qmax: -0.9682 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7049 | Qmax: -1.0753 | Exploration: 0.049016 \n",
      "| Reward: -43 | Episode: 7050 | Qmax: -0.7504 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7051 | Qmax: -0.8594 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7052 | Qmax: -0.7559 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7053 | Qmax: -0.7890 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7054 | Qmax: -0.7275 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7055 | Qmax: -0.6796 | Exploration: 0.049016 \n",
      "| Reward: -43 | Episode: 7056 | Qmax: -1.2258 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7057 | Qmax: -0.8853 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7058 | Qmax: -0.8096 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7059 | Qmax: -0.8804 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7060 | Qmax: -0.9580 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7061 | Qmax: -0.8019 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7062 | Qmax: -1.0036 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7063 | Qmax: -0.9176 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7064 | Qmax: -0.7093 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7065 | Qmax: -0.9150 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7066 | Qmax: -0.7956 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7067 | Qmax: -0.8210 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7068 | Qmax: -0.9732 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7069 | Qmax: -0.9258 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7070 | Qmax: -1.0436 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7071 | Qmax: -0.9120 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7072 | Qmax: -0.9876 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7073 | Qmax: -0.7896 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7074 | Qmax: -1.1737 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7075 | Qmax: -1.0114 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7076 | Qmax: -0.9043 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7077 | Qmax: -0.7999 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7078 | Qmax: -0.9873 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7079 | Qmax: -0.8656 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7080 | Qmax: -0.6914 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7081 | Qmax: -1.0474 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7082 | Qmax: -0.6759 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7083 | Qmax: -0.8159 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7084 | Qmax: -0.8265 | Exploration: 0.049016 \n",
      "| Reward: -45 | Episode: 7085 | Qmax: -0.8369 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7086 | Qmax: -0.9899 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7087 | Qmax: -0.8484 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7088 | Qmax: -0.7434 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7089 | Qmax: -1.0510 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7090 | Qmax: -0.7805 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7091 | Qmax: -0.8813 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7092 | Qmax: -1.0329 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7093 | Qmax: -0.8942 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7094 | Qmax: -0.7705 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7095 | Qmax: -0.7444 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7096 | Qmax: -0.7010 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7097 | Qmax: -0.6276 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7098 | Qmax: -0.5448 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7099 | Qmax: -0.7137 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7100 | Qmax: -0.9533 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7101 | Qmax: -0.8879 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7102 | Qmax: -0.5960 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7103 | Qmax: -0.7948 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7104 | Qmax: -0.4754 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7105 | Qmax: -1.1262 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7106 | Qmax: -1.1350 | Exploration: 0.049016 \n",
      "| Reward: -43 | Episode: 7107 | Qmax: -1.0088 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7108 | Qmax: -1.1333 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7109 | Qmax: -0.8494 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7110 | Qmax: -1.0508 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7111 | Qmax: -0.9383 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7112 | Qmax: -0.6231 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7113 | Qmax: -0.8047 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7114 | Qmax: -0.9784 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7115 | Qmax: -0.6936 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7116 | Qmax: -0.8520 | Exploration: 0.049016 \n",
      "| Reward: -43 | Episode: 7117 | Qmax: -0.7599 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7118 | Qmax: -1.1397 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7119 | Qmax: -1.1785 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7120 | Qmax: -0.8430 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7121 | Qmax: -0.8255 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7122 | Qmax: -0.7372 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7123 | Qmax: -0.8976 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7124 | Qmax: -0.9751 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7125 | Qmax: -0.9376 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7126 | Qmax: -0.9707 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7127 | Qmax: -0.8695 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7128 | Qmax: -0.6744 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7129 | Qmax: -0.5656 | Exploration: 0.049016 \n",
      "| Reward: -43 | Episode: 7130 | Qmax: -0.7975 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7131 | Qmax: -0.8560 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7132 | Qmax: -1.0126 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7133 | Qmax: -0.9622 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7134 | Qmax: -0.8124 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7135 | Qmax: -0.9543 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7136 | Qmax: -0.9933 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7137 | Qmax: -0.9849 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7138 | Qmax: -0.8790 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7139 | Qmax: -0.8044 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7140 | Qmax: -0.9814 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7141 | Qmax: -1.0066 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7142 | Qmax: -1.0367 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7143 | Qmax: -0.8408 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7144 | Qmax: -1.1655 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7145 | Qmax: -0.9192 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7146 | Qmax: -0.9881 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7147 | Qmax: -0.7773 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7148 | Qmax: -1.1303 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7149 | Qmax: -1.1021 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7150 | Qmax: -0.8374 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7151 | Qmax: -0.7543 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7152 | Qmax: -0.8436 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7153 | Qmax: -1.1750 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7154 | Qmax: -1.1142 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7155 | Qmax: -0.8830 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7156 | Qmax: -0.9014 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7157 | Qmax: -1.0509 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7158 | Qmax: -1.2552 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7159 | Qmax: -0.8757 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7160 | Qmax: -0.9817 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7161 | Qmax: -0.9455 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7162 | Qmax: -0.9742 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7163 | Qmax: -0.6524 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7164 | Qmax: -0.7100 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7165 | Qmax: -0.9280 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7166 | Qmax: -0.8908 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7167 | Qmax: -1.0351 | Exploration: 0.049016 \n",
      "| Reward: -43 | Episode: 7168 | Qmax: -0.8638 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7169 | Qmax: -1.1739 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7170 | Qmax: -1.0731 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7171 | Qmax: -1.0614 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7172 | Qmax: -0.7790 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7173 | Qmax: -0.6075 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7174 | Qmax: -1.1648 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7175 | Qmax: -0.6986 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7176 | Qmax: -0.6858 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7177 | Qmax: -1.0468 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7178 | Qmax: -1.0302 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7179 | Qmax: -1.1203 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7180 | Qmax: -0.8679 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7181 | Qmax: -0.7480 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7182 | Qmax: -0.8277 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7183 | Qmax: -0.7951 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7184 | Qmax: -0.8468 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7185 | Qmax: -0.7581 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7186 | Qmax: -0.7674 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7187 | Qmax: -0.9383 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7188 | Qmax: -0.8110 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7189 | Qmax: -0.9606 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7190 | Qmax: -0.8354 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7191 | Qmax: -0.8846 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7192 | Qmax: -0.6383 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7193 | Qmax: -0.9042 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7194 | Qmax: -0.8910 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7195 | Qmax: -0.9434 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7196 | Qmax: -1.0385 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7197 | Qmax: -0.9789 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7198 | Qmax: -1.0802 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7199 | Qmax: -0.9201 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7200 | Qmax: -0.9879 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7201 | Qmax: -0.6608 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7202 | Qmax: -0.4985 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7203 | Qmax: -0.6597 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7204 | Qmax: -0.6697 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7205 | Qmax: -0.9676 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7206 | Qmax: -0.8452 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7207 | Qmax: -1.1605 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7208 | Qmax: -0.6151 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7209 | Qmax: -1.0330 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7210 | Qmax: -0.8263 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7211 | Qmax: -1.2865 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7212 | Qmax: -1.0676 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7213 | Qmax: -0.5879 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7214 | Qmax: -0.8282 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7215 | Qmax: -0.6958 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7216 | Qmax: -0.7024 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7217 | Qmax: -0.7638 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7218 | Qmax: -0.9158 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7219 | Qmax: -1.1871 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 7220 | Qmax: -0.9961 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7221 | Qmax: -0.8493 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7222 | Qmax: -0.7391 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7223 | Qmax: -1.1115 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7224 | Qmax: -1.1896 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7225 | Qmax: -0.6862 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7226 | Qmax: -0.8678 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7227 | Qmax: -0.8729 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7228 | Qmax: -0.6526 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7229 | Qmax: -0.8187 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7230 | Qmax: -1.1898 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7231 | Qmax: -0.8324 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7232 | Qmax: -0.9070 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7233 | Qmax: -0.9403 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7234 | Qmax: -0.8061 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7235 | Qmax: -0.9877 | Exploration: 0.049016 \n",
      "| Reward: -41 | Episode: 7236 | Qmax: -1.0804 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7237 | Qmax: -1.1746 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7238 | Qmax: -1.0501 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7239 | Qmax: -1.0352 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7240 | Qmax: -0.9812 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7241 | Qmax: -0.7077 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 7242 | Qmax: -0.7222 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7243 | Qmax: -0.9512 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7244 | Qmax: -0.8088 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7245 | Qmax: -0.7419 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7246 | Qmax: -0.8182 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7247 | Qmax: -0.5962 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7248 | Qmax: -0.8409 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 7249 | Qmax: -1.0080 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 7250 | Qmax: -1.3579 | Exploration: 0.049016 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = CurrentWorld()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    env.seed(RANDOM_SEED)\n",
    "    \n",
    "    state_dim = 2\n",
    "    action_dim = 4\n",
    "    \n",
    "    Qnet = QNet(sess, state_dim, action_dim, LEARNING_RATE, TAU, MINIBATCH_SIZE, \"./saved_model/ddqn.ckpt\")\n",
    "    \n",
    "    train(sess, env, Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "world = np.zeros(env.shape)\n",
    "a_list = []\n",
    "s_list = []\n",
    "for s in range(env.nS):\n",
    "    a_list += [np.argmax(P[s])]\n",
    "    s_list += [np.unravel_index(s,env.shape)]\n",
    "for s,a in zip(s_list,a_list):\n",
    "    world[s] = a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib auto\n",
    "plt.imshow(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_optimal_path(Q,env):\n",
    "    env.reset()\n",
    "    start_state = env.start_state\n",
    "    terminal_state = env.terminal_state\n",
    "    state = np.ravel_multi_index(start_state,env.shape)\n",
    "    path = [start_state]\n",
    "    value = 0\n",
    "    action = []\n",
    "    while 1:\n",
    "        next_action = np.argmax(Q[state])\n",
    "        next_state,reward,done,_ = env.step(next_action)\n",
    "        path += [np.unravel_index(next_state,env.shape)]\n",
    "        value += reward\n",
    "        action += [next_action]\n",
    "        if done:\n",
    "            return path, action, value\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt_path,action,value = get_optimal_path(Q,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib auto\n",
    "world = deepcopy(env.winds)\n",
    "t = 0\n",
    "for i in opt_path[:-1]:\n",
    "    world[i] = 6\n",
    "#     world[i] += action[t]\n",
    "    t+=1\n",
    "plt.imshow(world)\n",
    "# print value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
