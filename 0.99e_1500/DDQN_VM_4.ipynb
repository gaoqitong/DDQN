{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from env_current import *\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed = 123):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "        \n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "    exploration_rate = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Exploration\", exploration_rate)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, exploration_rate]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNet(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, batch_size, save_path):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.inputs, self.q_values, self.a_predict = self.build_net()\n",
    "        self.net_params = tf.trainable_variables()\n",
    "        \n",
    "        self.target_inputs, self.target_q_values, self.target_a_predict = self.build_net()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params):]\n",
    "        \n",
    "        self.update_target_net_params = [self.target_net_params[i]\n",
    "                                         .assign(tf.multiply(self.tau, self.net_params[i])\n",
    "                                                 + tf.multiply((1.-self.tau), self.target_net_params[i]) ) \n",
    "                                         for i in range(len(self.target_net_params))]\n",
    "        \n",
    "        self.true_q_value = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "        \n",
    "        gather_indices = tf.range(MINIBATCH_SIZE) * tf.shape(self.q_values)[1] + tf.reshape(self.action, [-1])\n",
    "        self.action_correlated_q = tf.gather(tf.reshape(self.q_values,[-1]), gather_indices)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(tf.reshape(self.true_q_value, [-1]), self.action_correlated_q)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.last_num_epi = -1\n",
    "        \n",
    "    def build_net(self):\n",
    "        s_inputs = tf.placeholder(shape = [None, self.state_dim], dtype = tf.float32)\n",
    "        W1 = tf.Variable(tf.random_uniform([self.state_dim, 400], 0, 0.1))\n",
    "        B1 = tf.Variable(tf.zeros([400]))\n",
    "        L1 = tf.add(tf.matmul(s_inputs, W1), B1)\n",
    "        L1 = tf.layers.batch_normalization(L1)\n",
    "        L1 = tf.nn.relu(L1)\n",
    "        W2 = tf.Variable(tf.random_uniform([400, 300], 0, 0.1))\n",
    "        B2 = tf.Variable(tf.zeros([300]))\n",
    "        L2 = tf.add(tf.matmul(L1, W2), B2)\n",
    "        L2 = tf.layers.batch_normalization(L2)\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        W3 = tf.Variable(tf.random_uniform([300, self.action_dim], 0, 0.01))\n",
    "#         B3 = tf.Variable(tf.random_uniform([self.action_dim], -0.003, 0.003))\n",
    "#         q_values = tf.add(tf.matmul(L2, W3), B3)\n",
    "        q_values = tf.matmul(L2, W3)  \n",
    "        a_predict = tf.argmax(q_values,1)\n",
    "        \n",
    "        regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "        tf.contrib.layers.apply_regularization(regularizer,[W1, B1, W2, B2, W3])\n",
    "        return s_inputs, q_values, a_predict\n",
    "    \n",
    "    def train(self, states, action, true_q, num_epi):\n",
    "        if num_epi%20 == 0 and num_epi!=self.last_num_epi:\n",
    "            self.saver.save(self.sess, self.save_path)\n",
    "            print \"DDQN Saved\"\n",
    "            self.last_num_epi = num_epi\n",
    "            \n",
    "        return self.sess.run([self.q_values, self.optimizer], \n",
    "                             feed_dict={self.inputs: states, self.true_q_value: true_q, self.action: action})\n",
    "    \n",
    "    def predict_q(self, states):\n",
    "        return self.sess.run(self.q_values, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predict_a(self, states):\n",
    "        return self.sess.run(self.a_predict, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predect_target(self, states):\n",
    "        return self.sess.run(self.target_q_values, feed_dict={self.target_inputs: states})\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.sess.run(self.update_target_net_params)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, env, qnet):\n",
    "    \n",
    "    global EXPLORATION_RATE\n",
    "  \n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    \n",
    "    qnet.update_target()\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    for num_epi in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "        s = [list(np.unravel_index(s, env.shape))]\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "\n",
    "            a = np.argmax(qnet.predict_q(np.reshape(s, (1, qnet.state_dim))))\n",
    "    \n",
    "            if np.random.rand(1) < EXPLORATION_RATE:\n",
    "                s2, r, terminal, info = env.step(np.random.randint(0,qnet.action_dim))\n",
    "            else:\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "            \n",
    "            s2 = list(np.unravel_index(s2, env.shape))\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (qnet.state_dim,)), np.reshape(a, (1,)), r,\n",
    "                              terminal, np.reshape(s2, (qnet.state_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = qnet.predect_target(s2_batch)\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * np.amax(target_q[k]))\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = qnet.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)), num_epi)\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "                # Update target networks\n",
    "                qnet.update_target()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j == MAX_EPISODE_LEN-1:\n",
    "                \n",
    "                if EXPLORATION_RATE > 0.05 and terminal:\n",
    "                    EXPLORATION_RATE = EXPLORATION_RATE*0.99\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j),\n",
    "                    summary_vars[2]: EXPLORATION_RATE\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, num_epi)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} | Exploration: {:.6f} '.format(int(ep_reward), \\\n",
    "                        num_epi, (ep_ave_max_q / float(j)), EXPLORATION_RATE))\n",
    "                \n",
    "                f = open(\"stats.txt\", \"ab\")\n",
    "                f.write(\"| Reward: \" + str(int(ep_reward)) \n",
    "                        +\" | Episode: \" + str(num_epi) \n",
    "                        + \" | Qmax: \" + str(ep_ave_max_q / float(j)) \n",
    "                        + \" | Exploration: \" + str(EXPLORATION_RATE) + \"\\n\")\n",
    "                f.close()\n",
    "                \n",
    "                break\n",
    "                \n",
    "        if num_epi%1 == 0:\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            world = np.zeros(env.shape)\n",
    "            for state in range(env.nS):\n",
    "                state = np.unravel_index(state, env.shape)\n",
    "                action = qnet.predict_q(np.reshape(state, (1,state_dim)))\n",
    "                action = np.argmax(action)\n",
    "                state_list.append(state)\n",
    "                action_list.append(action)\n",
    "                \n",
    "#             print np.reshape(action_list, env.shape)\n",
    "                \n",
    "            f = open(\"action.txt\",\"ab\")\n",
    "            np.savetxt(f, np.reshape(action_list, env.shape), fmt=\"%i\")\n",
    "            f.write(\"---------------------------\\n\")\n",
    "            f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0015\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 10**6\n",
    "MINIBATCH_SIZE = 64\n",
    "RANDOM_SEED = 272\n",
    "MAX_EPISODES = 50000\n",
    "MAX_EPISODE_LEN = 1500\n",
    "SUMMARY_DIR = './results/tf_ddqn'\n",
    "EXPLORATION_RATE = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Saved\n",
      "| Reward: -1500 | Episode: 0 | Qmax: 132.7995 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 1 | Qmax: 172.7927 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 2 | Qmax: 171.1630 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 3 | Qmax: 167.2102 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 4 | Qmax: 162.6411 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 5 | Qmax: 158.0899 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 6 | Qmax: 153.3017 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 7 | Qmax: 148.7056 | Exploration: 0.650000 \n",
      "| Reward: -1500 | Episode: 8 | Qmax: 144.1715 | Exploration: 0.650000 \n",
      "| Reward: -119 | Episode: 9 | Qmax: 142.8914 | Exploration: 0.643500 \n",
      "| Reward: -95 | Episode: 10 | Qmax: 142.6651 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 11 | Qmax: 138.9362 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 12 | Qmax: 134.5085 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 13 | Qmax: 130.4402 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 14 | Qmax: 126.6143 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 15 | Qmax: 122.7806 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 16 | Qmax: 119.2316 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 17 | Qmax: 115.8428 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 18 | Qmax: 112.6046 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 19 | Qmax: 109.0621 | Exploration: 0.637065 \n",
      "DDQN Saved\n",
      "| Reward: -1500 | Episode: 20 | Qmax: 105.6001 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 21 | Qmax: 102.2862 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 22 | Qmax: 99.0271 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 23 | Qmax: 95.7580 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 24 | Qmax: 92.6334 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 25 | Qmax: 89.6260 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 26 | Qmax: 86.6034 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 27 | Qmax: 83.6731 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 28 | Qmax: 80.8557 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 29 | Qmax: 78.0825 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 30 | Qmax: 75.3325 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 31 | Qmax: 72.6214 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 32 | Qmax: 69.9299 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 33 | Qmax: 67.2985 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 34 | Qmax: 64.6725 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 35 | Qmax: 62.1004 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 36 | Qmax: 59.5600 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 37 | Qmax: 57.0290 | Exploration: 0.637065 \n",
      "| Reward: -1500 | Episode: 38 | Qmax: 54.5691 | Exploration: 0.637065 \n",
      "| Reward: -53 | Episode: 39 | Qmax: 54.3040 | Exploration: 0.630694 \n",
      "DDQN Saved\n",
      "| Reward: -1500 | Episode: 40 | Qmax: 52.0617 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 41 | Qmax: 49.6763 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 42 | Qmax: 47.3558 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 43 | Qmax: 45.0524 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 44 | Qmax: 42.7926 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 45 | Qmax: 40.5441 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 46 | Qmax: 38.3119 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 47 | Qmax: 36.1172 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 48 | Qmax: 33.9582 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 49 | Qmax: 31.8503 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 50 | Qmax: 29.7819 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 51 | Qmax: 27.7481 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 52 | Qmax: 25.7573 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 53 | Qmax: 23.7968 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 54 | Qmax: 21.8531 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 55 | Qmax: 19.9451 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 56 | Qmax: 18.0578 | Exploration: 0.630694 \n",
      "| Reward: -1500 | Episode: 57 | Qmax: 16.1925 | Exploration: 0.630694 \n",
      "| Reward: -121 | Episode: 58 | Qmax: 15.3058 | Exploration: 0.624387 \n",
      "| Reward: -1500 | Episode: 59 | Qmax: 14.2023 | Exploration: 0.624387 \n",
      "DDQN Saved\n",
      "| Reward: -1500 | Episode: 60 | Qmax: 12.3869 | Exploration: 0.624387 \n",
      "| Reward: -1500 | Episode: 61 | Qmax: 10.6037 | Exploration: 0.624387 \n",
      "| Reward: -1500 | Episode: 62 | Qmax: 8.8602 | Exploration: 0.624387 \n",
      "| Reward: -1500 | Episode: 63 | Qmax: 7.1393 | Exploration: 0.624387 \n",
      "| Reward: -1500 | Episode: 64 | Qmax: 5.4115 | Exploration: 0.624387 \n",
      "| Reward: -1500 | Episode: 65 | Qmax: 3.6886 | Exploration: 0.624387 \n",
      "| Reward: -91 | Episode: 66 | Qmax: 2.8075 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 67 | Qmax: 1.8715 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 68 | Qmax: 0.1969 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 69 | Qmax: -1.3887 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 70 | Qmax: -2.8974 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 71 | Qmax: -4.3161 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 72 | Qmax: -5.6567 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 73 | Qmax: -6.9590 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 74 | Qmax: -8.2588 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 75 | Qmax: -9.5478 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 76 | Qmax: -10.8266 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 77 | Qmax: -12.0886 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 78 | Qmax: -13.3363 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 79 | Qmax: -14.5860 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -1500 | Episode: 80 | Qmax: -15.8068 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 81 | Qmax: -17.0262 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 82 | Qmax: -18.2169 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 83 | Qmax: -19.3821 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 84 | Qmax: -20.5504 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 85 | Qmax: -21.6930 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 86 | Qmax: -22.8308 | Exploration: 0.618144 \n",
      "| Reward: -1500 | Episode: 87 | Qmax: -23.9572 | Exploration: 0.618144 \n",
      "| Reward: -43 | Episode: 5805 | Qmax: -1.5809 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5806 | Qmax: -1.7234 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5807 | Qmax: -1.5356 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5808 | Qmax: -1.3745 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5809 | Qmax: -1.3350 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5810 | Qmax: -1.5584 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5811 | Qmax: -1.1777 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5812 | Qmax: -1.5998 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5813 | Qmax: -1.6715 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5814 | Qmax: -1.2787 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5815 | Qmax: -1.1850 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5816 | Qmax: -1.7772 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5817 | Qmax: -1.6965 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5818 | Qmax: -1.6555 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5819 | Qmax: -1.1855 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 5820 | Qmax: -1.4467 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5821 | Qmax: -1.3484 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5822 | Qmax: -1.4349 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5823 | Qmax: -1.5581 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5824 | Qmax: -1.5720 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5825 | Qmax: -1.5464 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5826 | Qmax: -1.4344 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5827 | Qmax: -1.5962 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5828 | Qmax: -1.5512 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5829 | Qmax: -1.3526 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5830 | Qmax: -1.4611 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5831 | Qmax: -1.2693 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5832 | Qmax: -1.4746 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5833 | Qmax: -1.6154 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5834 | Qmax: -1.2964 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5835 | Qmax: -1.5320 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5836 | Qmax: -1.8123 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5837 | Qmax: -1.5613 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5838 | Qmax: -1.3443 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5839 | Qmax: -1.5839 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 5840 | Qmax: -1.6656 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5841 | Qmax: -1.4250 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5842 | Qmax: -1.4275 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5843 | Qmax: -1.8097 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5844 | Qmax: -1.5962 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5845 | Qmax: -1.4200 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5846 | Qmax: -1.3009 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5847 | Qmax: -1.4433 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5848 | Qmax: -1.6259 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5849 | Qmax: -1.4916 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5850 | Qmax: -1.5631 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5851 | Qmax: -1.4825 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5852 | Qmax: -1.7328 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5853 | Qmax: -1.6541 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5854 | Qmax: -1.5025 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5855 | Qmax: -1.4022 | Exploration: 0.049605 \n",
      "| Reward: -49 | Episode: 5856 | Qmax: -1.7223 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5857 | Qmax: -1.6615 | Exploration: 0.049605 \n",
      "| Reward: -49 | Episode: 5858 | Qmax: -1.6866 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5859 | Qmax: -1.6058 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 5860 | Qmax: -1.4007 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5861 | Qmax: -1.4876 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5862 | Qmax: -1.4275 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5863 | Qmax: -1.3546 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5864 | Qmax: -1.4341 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5865 | Qmax: -1.4390 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5866 | Qmax: -1.7434 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5867 | Qmax: -1.5195 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5868 | Qmax: -1.4137 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 5869 | Qmax: -1.6912 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5870 | Qmax: -1.7288 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5871 | Qmax: -1.4484 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5872 | Qmax: -1.4315 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5873 | Qmax: -1.3395 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5874 | Qmax: -1.4033 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5875 | Qmax: -1.3871 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5876 | Qmax: -1.5455 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5877 | Qmax: -1.5035 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5878 | Qmax: -1.5201 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5879 | Qmax: -1.2063 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 5880 | Qmax: -1.4357 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5881 | Qmax: -1.4012 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5882 | Qmax: -1.2088 | Exploration: 0.049605 \n",
      "| Reward: -45 | Episode: 5883 | Qmax: -1.9284 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5884 | Qmax: -1.2902 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5885 | Qmax: -1.6893 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5886 | Qmax: -1.2469 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5887 | Qmax: -1.7872 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5888 | Qmax: -1.3289 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5889 | Qmax: -1.3691 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5890 | Qmax: -1.2999 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5891 | Qmax: -1.5676 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5892 | Qmax: -1.5553 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5893 | Qmax: -1.6609 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5894 | Qmax: -1.8336 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5895 | Qmax: -1.8222 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5896 | Qmax: -1.4799 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5897 | Qmax: -1.7095 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5898 | Qmax: -1.2244 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5899 | Qmax: -1.2786 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 5900 | Qmax: -1.7778 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5901 | Qmax: -1.2971 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5902 | Qmax: -1.7922 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5903 | Qmax: -1.5330 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5904 | Qmax: -1.4471 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5905 | Qmax: -1.4717 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5906 | Qmax: -1.1532 | Exploration: 0.049605 \n",
      "| Reward: -57 | Episode: 5907 | Qmax: -1.7676 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5908 | Qmax: -1.5045 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5909 | Qmax: -1.0998 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5910 | Qmax: -1.1102 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5911 | Qmax: -1.6127 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5912 | Qmax: -1.3583 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5913 | Qmax: -1.2530 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5914 | Qmax: -1.4118 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5915 | Qmax: -1.5882 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5916 | Qmax: -1.4925 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5917 | Qmax: -1.2297 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5918 | Qmax: -1.6140 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5919 | Qmax: -1.4665 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -43 | Episode: 5920 | Qmax: -1.2224 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5921 | Qmax: -1.5784 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5922 | Qmax: -1.2630 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5923 | Qmax: -1.5051 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5924 | Qmax: -1.2930 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5925 | Qmax: -1.2865 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5926 | Qmax: -1.3955 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5927 | Qmax: -1.5645 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5928 | Qmax: -1.6474 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5929 | Qmax: -1.4250 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5930 | Qmax: -1.5581 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5931 | Qmax: -1.5689 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5932 | Qmax: -1.5770 | Exploration: 0.049605 \n",
      "| Reward: -45 | Episode: 5933 | Qmax: -1.4001 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5934 | Qmax: -1.5468 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5935 | Qmax: -1.3036 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5936 | Qmax: -1.7035 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5937 | Qmax: -1.8307 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5938 | Qmax: -1.1742 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5939 | Qmax: -1.3515 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 5940 | Qmax: -1.1561 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5941 | Qmax: -1.8036 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5942 | Qmax: -1.1966 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5943 | Qmax: -1.4808 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5944 | Qmax: -1.6124 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5945 | Qmax: -1.3890 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5946 | Qmax: -1.3418 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5947 | Qmax: -1.2789 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5948 | Qmax: -1.2955 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5949 | Qmax: -1.5403 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5950 | Qmax: -1.3716 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5951 | Qmax: -1.1883 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5952 | Qmax: -1.5525 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5953 | Qmax: -1.3870 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5954 | Qmax: -1.3200 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 5955 | Qmax: -1.3438 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5956 | Qmax: -1.1956 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5957 | Qmax: -1.3979 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5958 | Qmax: -1.5613 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5959 | Qmax: -1.5123 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 5960 | Qmax: -1.3485 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5961 | Qmax: -1.2785 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5962 | Qmax: -1.0554 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5963 | Qmax: -1.3076 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5964 | Qmax: -1.3750 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5965 | Qmax: -1.5097 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5966 | Qmax: -1.3191 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5967 | Qmax: -1.3976 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5968 | Qmax: -1.2414 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5969 | Qmax: -1.3025 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5970 | Qmax: -1.0928 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5971 | Qmax: -1.3371 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5972 | Qmax: -1.2420 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5973 | Qmax: -1.3836 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5974 | Qmax: -1.4125 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5975 | Qmax: -0.9603 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5976 | Qmax: -1.7325 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5977 | Qmax: -1.4285 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5978 | Qmax: -1.3805 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5979 | Qmax: -1.2381 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 5980 | Qmax: -1.4200 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5981 | Qmax: -1.7336 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5982 | Qmax: -1.1280 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5983 | Qmax: -1.1869 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5984 | Qmax: -1.7276 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5985 | Qmax: -1.3917 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5986 | Qmax: -1.1876 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 5987 | Qmax: -1.8946 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5988 | Qmax: -1.9242 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5989 | Qmax: -1.6187 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5990 | Qmax: -1.6405 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5991 | Qmax: -1.7806 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5992 | Qmax: -1.3701 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 5993 | Qmax: -1.5245 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5994 | Qmax: -1.2100 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 5995 | Qmax: -1.3847 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5996 | Qmax: -1.2356 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 5997 | Qmax: -1.4281 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 5998 | Qmax: -1.4894 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 5999 | Qmax: -1.4543 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 6000 | Qmax: -1.4842 | Exploration: 0.049605 \n",
      "| Reward: -49 | Episode: 6001 | Qmax: -1.4183 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6002 | Qmax: -1.4185 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6003 | Qmax: -1.4285 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6004 | Qmax: -1.4673 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6005 | Qmax: -1.3340 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6006 | Qmax: -1.3606 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6007 | Qmax: -1.6197 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6008 | Qmax: -1.3406 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6009 | Qmax: -1.1206 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6010 | Qmax: -1.4036 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 6011 | Qmax: -1.2628 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6012 | Qmax: -1.2852 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6013 | Qmax: -1.3091 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6014 | Qmax: -1.3104 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6015 | Qmax: -1.5635 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6016 | Qmax: -1.5299 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6017 | Qmax: -1.1482 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6018 | Qmax: -1.5960 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6019 | Qmax: -1.5304 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 6020 | Qmax: -1.4733 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 6021 | Qmax: -1.1578 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6022 | Qmax: -1.3338 | Exploration: 0.049605 \n",
      "| Reward: -47 | Episode: 6023 | Qmax: -1.3184 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6024 | Qmax: -1.3320 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6025 | Qmax: -1.2384 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6026 | Qmax: -1.1549 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6027 | Qmax: -1.1222 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 6028 | Qmax: -1.1645 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 6029 | Qmax: -1.2405 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6030 | Qmax: -1.2465 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6031 | Qmax: -1.3127 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6032 | Qmax: -1.2226 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 6033 | Qmax: -1.2489 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6034 | Qmax: -1.0802 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6035 | Qmax: -1.3536 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6036 | Qmax: -1.4533 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6037 | Qmax: -1.3043 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6038 | Qmax: -1.4589 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6039 | Qmax: -1.7645 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 6040 | Qmax: -1.4324 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6041 | Qmax: -1.1930 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6042 | Qmax: -1.7527 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6043 | Qmax: -1.4277 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6044 | Qmax: -1.2473 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6045 | Qmax: -1.3509 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6046 | Qmax: -1.5092 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6047 | Qmax: -1.6068 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6048 | Qmax: -1.2164 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6049 | Qmax: -1.5648 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6050 | Qmax: -1.1756 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6051 | Qmax: -1.6115 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6052 | Qmax: -1.5505 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6053 | Qmax: -1.5963 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6054 | Qmax: -1.4768 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6055 | Qmax: -1.1359 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6056 | Qmax: -1.4911 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6057 | Qmax: -1.4332 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6058 | Qmax: -1.5528 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6059 | Qmax: -1.4990 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 6060 | Qmax: -1.4807 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6061 | Qmax: -1.5226 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6062 | Qmax: -1.1698 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 6063 | Qmax: -1.6546 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 6064 | Qmax: -1.2892 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6065 | Qmax: -1.2705 | Exploration: 0.049605 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = CurrentWorld()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    env.seed(RANDOM_SEED)\n",
    "    \n",
    "    state_dim = 2\n",
    "    action_dim = 4\n",
    "    \n",
    "    Qnet = QNet(sess, state_dim, action_dim, LEARNING_RATE, TAU, MINIBATCH_SIZE, \"./saved_model/ddqn.ckpt\")\n",
    "    \n",
    "    train(sess, env, Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "world = np.zeros(env.shape)\n",
    "a_list = []\n",
    "s_list = []\n",
    "for s in range(env.nS):\n",
    "    a_list += [np.argmax(P[s])]\n",
    "    s_list += [np.unravel_index(s,env.shape)]\n",
    "for s,a in zip(s_list,a_list):\n",
    "    world[s] = a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib auto\n",
    "plt.imshow(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_optimal_path(Q,env):\n",
    "    env.reset()\n",
    "    start_state = env.start_state\n",
    "    terminal_state = env.terminal_state\n",
    "    state = np.ravel_multi_index(start_state,env.shape)\n",
    "    path = [start_state]\n",
    "    value = 0\n",
    "    action = []\n",
    "    while 1:\n",
    "        next_action = np.argmax(Q[state])\n",
    "        next_state,reward,done,_ = env.step(next_action)\n",
    "        path += [np.unravel_index(next_state,env.shape)]\n",
    "        value += reward\n",
    "        action += [next_action]\n",
    "        if done:\n",
    "            return path, action, value\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt_path,action,value = get_optimal_path(Q,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib auto\n",
    "world = deepcopy(env.winds)\n",
    "t = 0\n",
    "for i in opt_path[:-1]:\n",
    "    world[i] = 6\n",
    "#     world[i] += action[t]\n",
    "    t+=1\n",
    "plt.imshow(world)\n",
    "# print value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
