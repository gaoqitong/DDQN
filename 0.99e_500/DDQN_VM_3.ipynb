{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from env_current import *\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed = 123):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "        \n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "    exploration_rate = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Exploration\", exploration_rate)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, exploration_rate]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNet(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, batch_size, save_path):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.inputs, self.q_values, self.a_predict = self.build_net()\n",
    "        self.net_params = tf.trainable_variables()\n",
    "        \n",
    "        self.target_inputs, self.target_q_values, self.target_a_predict = self.build_net()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params):]\n",
    "        \n",
    "        self.update_target_net_params = [self.target_net_params[i]\n",
    "                                         .assign(tf.multiply(self.tau, self.net_params[i])\n",
    "                                                 + tf.multiply((1.-self.tau), self.target_net_params[i]) ) \n",
    "                                         for i in range(len(self.target_net_params))]\n",
    "        \n",
    "        self.true_q_value = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "        \n",
    "        gather_indices = tf.range(MINIBATCH_SIZE) * tf.shape(self.q_values)[1] + tf.reshape(self.action, [-1])\n",
    "        self.action_correlated_q = tf.gather(tf.reshape(self.q_values,[-1]), gather_indices)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(tf.reshape(self.true_q_value, [-1]), self.action_correlated_q)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.last_num_epi = -1\n",
    "        \n",
    "    def build_net(self):\n",
    "        s_inputs = tf.placeholder(shape = [None, self.state_dim], dtype = tf.float32)\n",
    "        W1 = tf.Variable(tf.random_uniform([self.state_dim, 400], 0, 0.1))\n",
    "        B1 = tf.Variable(tf.zeros([400]))\n",
    "        L1 = tf.add(tf.matmul(s_inputs, W1), B1)\n",
    "        L1 = tf.layers.batch_normalization(L1)\n",
    "        L1 = tf.nn.relu(L1)\n",
    "        W2 = tf.Variable(tf.random_uniform([400, 300], 0, 0.1))\n",
    "        B2 = tf.Variable(tf.zeros([300]))\n",
    "        L2 = tf.add(tf.matmul(L1, W2), B2)\n",
    "        L2 = tf.layers.batch_normalization(L2)\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        W3 = tf.Variable(tf.random_uniform([300, self.action_dim], 0, 0.01))\n",
    "#         B3 = tf.Variable(tf.random_uniform([self.action_dim], -0.003, 0.003))\n",
    "#         q_values = tf.add(tf.matmul(L2, W3), B3)\n",
    "        q_values = tf.matmul(L2, W3)  \n",
    "        a_predict = tf.argmax(q_values,1)\n",
    "        \n",
    "        regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "        tf.contrib.layers.apply_regularization(regularizer,[W1, B1, W2, B2, W3])\n",
    "        return s_inputs, q_values, a_predict\n",
    "    \n",
    "    def train(self, states, action, true_q, num_epi):\n",
    "        if num_epi%20 == 0 and num_epi!=self.last_num_epi:\n",
    "            self.saver.save(self.sess, self.save_path)\n",
    "            print \"DDQN Saved\"\n",
    "            self.last_num_epi = num_epi\n",
    "            \n",
    "        return self.sess.run([self.q_values, self.optimizer], \n",
    "                             feed_dict={self.inputs: states, self.true_q_value: true_q, self.action: action})\n",
    "    \n",
    "    def predict_q(self, states):\n",
    "        return self.sess.run(self.q_values, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predict_a(self, states):\n",
    "        return self.sess.run(self.a_predict, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predect_target(self, states):\n",
    "        return self.sess.run(self.target_q_values, feed_dict={self.target_inputs: states})\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.sess.run(self.update_target_net_params)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, env, qnet):\n",
    "    \n",
    "    global EXPLORATION_RATE\n",
    "  \n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    \n",
    "    qnet.update_target()\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    for num_epi in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "        s = [list(np.unravel_index(s, env.shape))]\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "\n",
    "            a = np.argmax(qnet.predict_q(np.reshape(s, (1, qnet.state_dim))))\n",
    "    \n",
    "            if np.random.rand(1) < EXPLORATION_RATE:\n",
    "                s2, r, terminal, info = env.step(np.random.randint(0,qnet.action_dim))\n",
    "            else:\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "            \n",
    "            s2 = list(np.unravel_index(s2, env.shape))\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (qnet.state_dim,)), np.reshape(a, (1,)), r,\n",
    "                              terminal, np.reshape(s2, (qnet.state_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = qnet.predect_target(s2_batch)\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * np.amax(target_q[k]))\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = qnet.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)), num_epi)\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "                # Update target networks\n",
    "                qnet.update_target()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j == MAX_EPISODE_LEN-1:\n",
    "                \n",
    "                if EXPLORATION_RATE > 0.05 and terminal:\n",
    "                    EXPLORATION_RATE = EXPLORATION_RATE*0.99\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j),\n",
    "                    summary_vars[2]: EXPLORATION_RATE\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, num_epi)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} | Exploration: {:.6f} '.format(int(ep_reward), \\\n",
    "                        num_epi, (ep_ave_max_q / float(j)), EXPLORATION_RATE))\n",
    "                \n",
    "                f = open(\"stats.txt\", \"ab\")\n",
    "                f.write(\"| Reward: \" + str(int(ep_reward)) \n",
    "                        +\" | Episode: \" + str(num_epi) \n",
    "                        + \" | Qmax: \" + str(ep_ave_max_q / float(j)) \n",
    "                        + \" | Exploration: \" + str(EXPLORATION_RATE) + \"\\n\")\n",
    "                f.close()\n",
    "                \n",
    "                break\n",
    "                \n",
    "        if num_epi%1 == 0:\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            world = np.zeros(env.shape)\n",
    "            for state in range(env.nS):\n",
    "                state = np.unravel_index(state, env.shape)\n",
    "                action = qnet.predict_q(np.reshape(state, (1,state_dim)))\n",
    "                action = np.argmax(action)\n",
    "                state_list.append(state)\n",
    "                action_list.append(action)\n",
    "                \n",
    "#             print np.reshape(action_list, env.shape)\n",
    "                \n",
    "            f = open(\"action.txt\",\"ab\")\n",
    "            np.savetxt(f, np.reshape(action_list, env.shape), fmt=\"%i\")\n",
    "            f.write(\"---------------------------\\n\")\n",
    "            f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0015\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 10**6\n",
    "MINIBATCH_SIZE = 64\n",
    "RANDOM_SEED = 272\n",
    "MAX_EPISODES = 50000\n",
    "MAX_EPISODE_LEN = 500\n",
    "SUMMARY_DIR = './results/tf_ddqn'\n",
    "EXPLORATION_RATE = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 0 | Qmax: 84.7947 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 1 | Qmax: 143.6896 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 2 | Qmax: 159.7251 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 3 | Qmax: 172.0265 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 4 | Qmax: 173.8788 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 5 | Qmax: 173.8859 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 6 | Qmax: 174.0377 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 7 | Qmax: 172.8736 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 8 | Qmax: 171.6056 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 9 | Qmax: 170.7364 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 10 | Qmax: 169.2289 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 11 | Qmax: 167.6618 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 12 | Qmax: 166.0959 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 13 | Qmax: 164.5093 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 14 | Qmax: 163.0243 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 15 | Qmax: 161.4573 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 16 | Qmax: 159.7517 | Exploration: 0.650000 \n",
      "| Reward: -500 | Episode: 17 | Qmax: 157.9788 | Exploration: 0.650000 \n",
      "| Reward: -109 | Episode: 18 | Qmax: 158.2855 | Exploration: 0.643500 \n",
      "| Reward: -500 | Episode: 19 | Qmax: 155.9045 | Exploration: 0.643500 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 20 | Qmax: 154.2049 | Exploration: 0.643500 \n",
      "| Reward: -500 | Episode: 21 | Qmax: 152.4428 | Exploration: 0.643500 \n",
      "| Reward: -117 | Episode: 22 | Qmax: 152.4708 | Exploration: 0.637065 \n",
      "| Reward: -500 | Episode: 23 | Qmax: 150.5851 | Exploration: 0.637065 \n",
      "| Reward: -500 | Episode: 24 | Qmax: 148.9765 | Exploration: 0.637065 \n",
      "| Reward: -500 | Episode: 25 | Qmax: 147.4906 | Exploration: 0.637065 \n",
      "| Reward: -500 | Episode: 26 | Qmax: 145.8435 | Exploration: 0.637065 \n",
      "| Reward: -125 | Episode: 27 | Qmax: 145.7470 | Exploration: 0.630694 \n",
      "| Reward: -500 | Episode: 28 | Qmax: 143.9001 | Exploration: 0.630694 \n",
      "| Reward: -105 | Episode: 29 | Qmax: 144.2409 | Exploration: 0.624387 \n",
      "| Reward: -500 | Episode: 30 | Qmax: 142.1336 | Exploration: 0.624387 \n",
      "| Reward: -500 | Episode: 31 | Qmax: 140.7570 | Exploration: 0.624387 \n",
      "| Reward: -500 | Episode: 32 | Qmax: 139.5403 | Exploration: 0.624387 \n",
      "| Reward: -500 | Episode: 33 | Qmax: 137.8701 | Exploration: 0.624387 \n",
      "| Reward: -115 | Episode: 34 | Qmax: 137.9211 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 35 | Qmax: 136.0595 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 36 | Qmax: 134.6808 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 37 | Qmax: 133.3280 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 38 | Qmax: 131.8696 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 39 | Qmax: 130.5697 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 40 | Qmax: 129.3036 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 41 | Qmax: 128.0600 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 42 | Qmax: 126.7933 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 43 | Qmax: 125.5172 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 44 | Qmax: 124.2762 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 45 | Qmax: 123.0456 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 46 | Qmax: 121.7701 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 47 | Qmax: 120.6273 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 48 | Qmax: 119.2582 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 49 | Qmax: 117.9426 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 50 | Qmax: 116.7591 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 51 | Qmax: 115.5563 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 52 | Qmax: 114.4009 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 53 | Qmax: 113.2026 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 54 | Qmax: 112.0801 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 55 | Qmax: 110.8448 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 56 | Qmax: 109.6986 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 57 | Qmax: 108.6188 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 58 | Qmax: 107.4679 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 59 | Qmax: 106.3403 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 60 | Qmax: 105.2113 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 61 | Qmax: 104.1354 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 62 | Qmax: 103.0587 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 63 | Qmax: 101.9631 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 64 | Qmax: 100.8603 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 65 | Qmax: 99.8206 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 66 | Qmax: 98.7723 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 67 | Qmax: 97.6722 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 68 | Qmax: 96.6348 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 69 | Qmax: 95.5323 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 70 | Qmax: 94.4466 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 71 | Qmax: 93.2063 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 72 | Qmax: 92.1173 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 73 | Qmax: 90.9515 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 74 | Qmax: 89.9177 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 75 | Qmax: 88.8893 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 76 | Qmax: 87.8999 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 77 | Qmax: 86.9456 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 78 | Qmax: 86.0185 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 79 | Qmax: 85.0109 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 80 | Qmax: 84.0835 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 81 | Qmax: 83.1561 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 82 | Qmax: 82.1619 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 83 | Qmax: 81.2614 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 84 | Qmax: 80.2976 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 85 | Qmax: 79.3692 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 86 | Qmax: 78.5045 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 87 | Qmax: 77.5490 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 88 | Qmax: 76.6526 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 89 | Qmax: 75.6963 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 90 | Qmax: 74.7939 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 91 | Qmax: 73.8816 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 92 | Qmax: 72.9879 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 93 | Qmax: 72.0970 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 94 | Qmax: 71.1941 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 95 | Qmax: 70.2846 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 96 | Qmax: 69.4035 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 97 | Qmax: 68.5532 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 98 | Qmax: 67.6231 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 99 | Qmax: 66.7872 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 100 | Qmax: 65.9011 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 101 | Qmax: 65.0196 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 102 | Qmax: 64.1872 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 103 | Qmax: 63.3869 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 104 | Qmax: 62.5502 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 105 | Qmax: 61.7130 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 106 | Qmax: 60.8791 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 107 | Qmax: 60.0370 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 108 | Qmax: 59.1993 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 109 | Qmax: 58.3867 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 110 | Qmax: 57.5820 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 111 | Qmax: 56.7216 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 112 | Qmax: 55.9417 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 113 | Qmax: 55.0999 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 114 | Qmax: 54.2902 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 115 | Qmax: 53.4755 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 116 | Qmax: 52.6742 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 117 | Qmax: 51.8753 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 118 | Qmax: 51.0960 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 119 | Qmax: 50.2746 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 120 | Qmax: 49.5235 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 121 | Qmax: 48.7087 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 122 | Qmax: 47.9581 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 123 | Qmax: 47.1568 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 124 | Qmax: 46.3994 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 125 | Qmax: 45.6376 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 126 | Qmax: 44.8460 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 127 | Qmax: 44.0849 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 128 | Qmax: 43.3239 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 129 | Qmax: 42.5958 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 130 | Qmax: 41.8391 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 131 | Qmax: 41.0998 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 132 | Qmax: 40.3636 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 133 | Qmax: 39.6386 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 134 | Qmax: 38.9274 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 135 | Qmax: 38.2047 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 136 | Qmax: 37.4898 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 137 | Qmax: 36.7911 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 138 | Qmax: 36.0941 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 139 | Qmax: 35.3768 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 140 | Qmax: 34.6580 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 141 | Qmax: 33.9691 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 142 | Qmax: 33.2685 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 143 | Qmax: 32.5883 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 144 | Qmax: 31.8888 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 145 | Qmax: 31.2091 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 146 | Qmax: 30.5348 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 147 | Qmax: 29.8458 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 148 | Qmax: 29.1647 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 149 | Qmax: 28.4954 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 150 | Qmax: 27.8369 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 151 | Qmax: 27.1700 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 152 | Qmax: 26.5053 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 153 | Qmax: 25.8337 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 154 | Qmax: 25.1724 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 155 | Qmax: 24.5194 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 156 | Qmax: 23.8837 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 157 | Qmax: 23.2359 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 158 | Qmax: 22.5829 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 159 | Qmax: 21.9494 | Exploration: 0.618144 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 160 | Qmax: 21.3029 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 161 | Qmax: 20.6650 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 162 | Qmax: 20.0230 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 163 | Qmax: 19.3931 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 164 | Qmax: 18.7585 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 165 | Qmax: 18.1276 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 166 | Qmax: 17.5083 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 167 | Qmax: 16.8759 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 168 | Qmax: 16.2654 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 169 | Qmax: 15.6488 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 170 | Qmax: 15.0444 | Exploration: 0.618144 \n",
      "| Reward: -500 | Episode: 171 | Qmax: 14.4326 | Exploration: 0.618144 \n",
      "| Reward: -89 | Episode: 172 | Qmax: 14.2085 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 173 | Qmax: 13.7288 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 174 | Qmax: 13.1309 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 175 | Qmax: 12.5300 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 176 | Qmax: 11.9385 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 177 | Qmax: 11.3523 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 178 | Qmax: 10.7535 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 179 | Qmax: 10.1625 | Exploration: 0.611962 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 180 | Qmax: 9.5781 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 181 | Qmax: 8.9935 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 182 | Qmax: 8.3989 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 183 | Qmax: 7.8125 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 184 | Qmax: 7.2274 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 185 | Qmax: 6.6470 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 186 | Qmax: 6.0693 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 187 | Qmax: 5.4916 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 188 | Qmax: 4.9256 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 189 | Qmax: 4.3468 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 190 | Qmax: 3.7777 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 191 | Qmax: 3.2067 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 192 | Qmax: 2.6319 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 193 | Qmax: 2.0688 | Exploration: 0.611962 \n",
      "| Reward: -500 | Episode: 194 | Qmax: 1.5021 | Exploration: 0.611962 \n",
      "| Reward: -123 | Episode: 195 | Qmax: 1.1539 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 196 | Qmax: 0.7979 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 197 | Qmax: 0.2400 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 198 | Qmax: -0.3171 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 199 | Qmax: -0.8659 | Exploration: 0.605842 \n",
      "DDQN Saved\n",
      "| Reward: -500 | Episode: 200 | Qmax: -1.4150 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 201 | Qmax: -1.9432 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 202 | Qmax: -2.4381 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 203 | Qmax: -2.9319 | Exploration: 0.605842 \n",
      "| Reward: -500 | Episode: 204 | Qmax: -3.4143 | Exploration: 0.605842 \n",
      "| Reward: -85 | Episode: 205 | Qmax: -3.7348 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 206 | Qmax: -3.9607 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 207 | Qmax: -4.4132 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 208 | Qmax: -4.8749 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 209 | Qmax: -5.3340 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 210 | Qmax: -5.7860 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 211 | Qmax: -6.2270 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 212 | Qmax: -6.6657 | Exploration: 0.599784 \n",
      "| Reward: -500 | Episode: 213 | Qmax: -7.0987 | Exploration: 0.599784 \n",
      "| Reward: -39 | Episode: 6912 | Qmax: -1.0226 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6913 | Qmax: -0.9554 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6914 | Qmax: -0.9002 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6915 | Qmax: -1.0629 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6916 | Qmax: -0.7485 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6917 | Qmax: -0.9468 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6918 | Qmax: -0.9429 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6919 | Qmax: -1.1075 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 6920 | Qmax: -1.1528 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6921 | Qmax: -0.8550 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6922 | Qmax: -0.7084 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6923 | Qmax: -1.2993 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6924 | Qmax: -0.9975 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6925 | Qmax: -0.8741 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6926 | Qmax: -1.0956 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6927 | Qmax: -0.7016 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6928 | Qmax: -1.0320 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6929 | Qmax: -0.9202 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6930 | Qmax: -0.8801 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6931 | Qmax: -1.0181 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6932 | Qmax: -0.6470 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6933 | Qmax: -0.8962 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6934 | Qmax: -0.7337 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6935 | Qmax: -0.9525 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6936 | Qmax: -0.8570 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6937 | Qmax: -1.0324 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6938 | Qmax: -0.8388 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6939 | Qmax: -0.8967 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 6940 | Qmax: -0.8012 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6941 | Qmax: -0.8036 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6942 | Qmax: -0.8106 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6943 | Qmax: -0.8994 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6944 | Qmax: -1.0504 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6945 | Qmax: -1.0902 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 6946 | Qmax: -0.8856 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6947 | Qmax: -0.9081 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 6948 | Qmax: -0.9519 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6949 | Qmax: -0.5357 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6950 | Qmax: -1.0258 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6951 | Qmax: -0.8485 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6952 | Qmax: -1.0690 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6953 | Qmax: -0.7240 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6954 | Qmax: -1.0819 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6955 | Qmax: -0.8849 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6956 | Qmax: -0.9871 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6957 | Qmax: -0.9132 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6958 | Qmax: -1.0011 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6959 | Qmax: -1.1748 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 6960 | Qmax: -0.9442 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6961 | Qmax: -1.3056 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6962 | Qmax: -0.7348 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6963 | Qmax: -1.0009 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6964 | Qmax: -0.7453 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6965 | Qmax: -0.9598 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6966 | Qmax: -0.9227 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6967 | Qmax: -0.8892 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6968 | Qmax: -0.6746 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6969 | Qmax: -0.6830 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6970 | Qmax: -0.8462 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 6971 | Qmax: -1.2133 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6972 | Qmax: -0.9543 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6973 | Qmax: -0.9183 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6974 | Qmax: -0.9797 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6975 | Qmax: -0.9584 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6976 | Qmax: -0.9929 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6977 | Qmax: -0.9357 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6978 | Qmax: -0.6980 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6979 | Qmax: -1.0572 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 6980 | Qmax: -0.7421 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6981 | Qmax: -1.2819 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6982 | Qmax: -0.9929 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6983 | Qmax: -1.1708 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6984 | Qmax: -0.6587 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6985 | Qmax: -1.2722 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6986 | Qmax: -0.7625 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6987 | Qmax: -0.9356 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6988 | Qmax: -1.0796 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6989 | Qmax: -1.2429 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6990 | Qmax: -0.9356 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6991 | Qmax: -0.8497 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6992 | Qmax: -1.0027 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 6993 | Qmax: -0.9724 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6994 | Qmax: -0.8010 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 6995 | Qmax: -0.8013 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6996 | Qmax: -0.8632 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 6997 | Qmax: -0.8667 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 6998 | Qmax: -0.7926 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 6999 | Qmax: -0.9585 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7000 | Qmax: -0.7655 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7001 | Qmax: -0.9331 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7002 | Qmax: -0.9962 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7003 | Qmax: -0.7824 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7004 | Qmax: -0.7321 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7005 | Qmax: -0.9546 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7006 | Qmax: -0.8886 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7007 | Qmax: -0.8826 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7008 | Qmax: -0.6703 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7009 | Qmax: -1.0452 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7010 | Qmax: -0.8590 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7011 | Qmax: -0.8722 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7012 | Qmax: -0.9551 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7013 | Qmax: -0.8030 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7014 | Qmax: -0.8535 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7015 | Qmax: -0.9635 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7016 | Qmax: -0.8694 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7017 | Qmax: -0.8631 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7018 | Qmax: -0.8761 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7019 | Qmax: -1.0687 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7020 | Qmax: -0.6839 | Exploration: 0.049605 \n",
      "| Reward: -43 | Episode: 7021 | Qmax: -1.1660 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7022 | Qmax: -1.0254 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7023 | Qmax: -0.9532 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7024 | Qmax: -0.7715 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7025 | Qmax: -1.2251 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7026 | Qmax: -0.8058 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7027 | Qmax: -0.6316 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7028 | Qmax: -1.1996 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7029 | Qmax: -0.6263 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7030 | Qmax: -1.0034 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7031 | Qmax: -0.9706 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7032 | Qmax: -1.0254 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7033 | Qmax: -1.2737 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7034 | Qmax: -1.1131 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7035 | Qmax: -0.7028 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7036 | Qmax: -1.0388 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7037 | Qmax: -0.9290 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7038 | Qmax: -1.0578 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7039 | Qmax: -0.6650 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7040 | Qmax: -0.8847 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7041 | Qmax: -0.9933 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7042 | Qmax: -1.4166 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7043 | Qmax: -0.9749 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7044 | Qmax: -0.5261 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7045 | Qmax: -0.8130 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7046 | Qmax: -0.9842 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7047 | Qmax: -0.8548 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7048 | Qmax: -0.4470 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7049 | Qmax: -0.9153 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7050 | Qmax: -0.9366 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7051 | Qmax: -0.7417 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7052 | Qmax: -1.0453 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7053 | Qmax: -1.2940 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7054 | Qmax: -0.5589 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7055 | Qmax: -1.0338 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7056 | Qmax: -1.2005 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7057 | Qmax: -0.5379 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7058 | Qmax: -0.8591 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7059 | Qmax: -0.9416 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 7060 | Qmax: -0.9531 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7061 | Qmax: -0.8111 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7062 | Qmax: -1.0575 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7063 | Qmax: -0.6925 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7064 | Qmax: -0.8434 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7065 | Qmax: -0.7713 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7066 | Qmax: -0.7209 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7067 | Qmax: -0.8403 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7068 | Qmax: -1.3028 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7069 | Qmax: -0.8787 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7070 | Qmax: -1.0124 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7071 | Qmax: -0.9352 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7072 | Qmax: -1.0120 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7073 | Qmax: -1.0754 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7074 | Qmax: -1.1438 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7075 | Qmax: -0.8420 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7076 | Qmax: -0.9099 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7077 | Qmax: -0.5622 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7078 | Qmax: -0.6636 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7079 | Qmax: -1.2683 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7080 | Qmax: -0.7318 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7081 | Qmax: -0.8450 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7082 | Qmax: -1.0976 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7083 | Qmax: -0.9081 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7084 | Qmax: -0.7939 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7085 | Qmax: -0.9444 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7086 | Qmax: -0.8601 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7087 | Qmax: -1.0686 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7088 | Qmax: -1.3075 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7089 | Qmax: -0.8531 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7090 | Qmax: -0.9515 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7091 | Qmax: -0.8802 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7092 | Qmax: -0.8393 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7093 | Qmax: -1.0597 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7094 | Qmax: -0.8783 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7095 | Qmax: -0.9894 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7096 | Qmax: -0.7000 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7097 | Qmax: -0.9437 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7098 | Qmax: -0.9876 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7099 | Qmax: -0.9456 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7100 | Qmax: -0.7730 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7101 | Qmax: -0.7292 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7102 | Qmax: -0.7819 | Exploration: 0.049605 \n",
      "| Reward: -45 | Episode: 7103 | Qmax: -0.8290 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7104 | Qmax: -0.9185 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7105 | Qmax: -1.1030 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7106 | Qmax: -1.1632 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7107 | Qmax: -0.8527 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7108 | Qmax: -0.6893 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7109 | Qmax: -0.8489 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7110 | Qmax: -1.0856 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7111 | Qmax: -0.7847 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7112 | Qmax: -1.0328 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7113 | Qmax: -1.0850 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7114 | Qmax: -0.9132 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7115 | Qmax: -0.8391 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7116 | Qmax: -0.9697 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7117 | Qmax: -0.8165 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7118 | Qmax: -0.7522 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7119 | Qmax: -0.8841 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7120 | Qmax: -0.8478 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7121 | Qmax: -0.9870 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7122 | Qmax: -0.8624 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7123 | Qmax: -1.0187 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7124 | Qmax: -1.0980 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7125 | Qmax: -0.7116 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7126 | Qmax: -0.8088 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7127 | Qmax: -0.9467 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7128 | Qmax: -0.8402 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7129 | Qmax: -1.1800 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7130 | Qmax: -0.8469 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7131 | Qmax: -1.0089 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7132 | Qmax: -0.9371 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7133 | Qmax: -0.7612 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7134 | Qmax: -0.8252 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7135 | Qmax: -0.8125 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7136 | Qmax: -0.8248 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7137 | Qmax: -1.0628 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7138 | Qmax: -0.8740 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7139 | Qmax: -0.9766 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -37 | Episode: 7140 | Qmax: -0.9118 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7141 | Qmax: -0.8610 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7142 | Qmax: -0.5121 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7143 | Qmax: -0.9183 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7144 | Qmax: -0.8817 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7145 | Qmax: -0.6775 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7146 | Qmax: -0.9123 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7147 | Qmax: -0.9818 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7148 | Qmax: -1.1989 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7149 | Qmax: -0.7780 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7150 | Qmax: -0.6921 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7151 | Qmax: -0.8611 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7152 | Qmax: -0.9166 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7153 | Qmax: -0.8512 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7154 | Qmax: -1.0281 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7155 | Qmax: -0.9586 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7156 | Qmax: -1.0380 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7157 | Qmax: -0.7790 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7158 | Qmax: -0.9137 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7159 | Qmax: -0.8628 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 7160 | Qmax: -0.7220 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7161 | Qmax: -0.9323 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7162 | Qmax: -0.7699 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7163 | Qmax: -0.7659 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7164 | Qmax: -1.1954 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7165 | Qmax: -0.8380 | Exploration: 0.049605 \n",
      "| Reward: -39 | Episode: 7166 | Qmax: -0.7185 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7167 | Qmax: -0.7073 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7168 | Qmax: -0.8218 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7169 | Qmax: -0.7735 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7170 | Qmax: -1.0920 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7171 | Qmax: -0.8421 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7172 | Qmax: -1.2597 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7173 | Qmax: -0.6531 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7174 | Qmax: -1.0023 | Exploration: 0.049605 \n",
      "| Reward: -41 | Episode: 7175 | Qmax: -1.2368 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7176 | Qmax: -0.5218 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7177 | Qmax: -1.1877 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7178 | Qmax: -0.9758 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7179 | Qmax: -1.2342 | Exploration: 0.049605 \n",
      "DDQN Saved\n",
      "| Reward: -39 | Episode: 7180 | Qmax: -1.2072 | Exploration: 0.049605 \n",
      "| Reward: -37 | Episode: 7181 | Qmax: -1.1660 | Exploration: 0.049605 \n",
      "| Reward: -35 | Episode: 7182 | Qmax: -0.9388 | Exploration: 0.049605 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = CurrentWorld()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    env.seed(RANDOM_SEED)\n",
    "    \n",
    "    state_dim = 2\n",
    "    action_dim = 4\n",
    "    \n",
    "    Qnet = QNet(sess, state_dim, action_dim, LEARNING_RATE, TAU, MINIBATCH_SIZE, \"./saved_model/ddqn.ckpt\")\n",
    "    \n",
    "    train(sess, env, Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "world = np.zeros(env.shape)\n",
    "a_list = []\n",
    "s_list = []\n",
    "for s in range(env.nS):\n",
    "    a_list += [np.argmax(P[s])]\n",
    "    s_list += [np.unravel_index(s,env.shape)]\n",
    "for s,a in zip(s_list,a_list):\n",
    "    world[s] = a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib auto\n",
    "plt.imshow(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_optimal_path(Q,env):\n",
    "    env.reset()\n",
    "    start_state = env.start_state\n",
    "    terminal_state = env.terminal_state\n",
    "    state = np.ravel_multi_index(start_state,env.shape)\n",
    "    path = [start_state]\n",
    "    value = 0\n",
    "    action = []\n",
    "    while 1:\n",
    "        next_action = np.argmax(Q[state])\n",
    "        next_state,reward,done,_ = env.step(next_action)\n",
    "        path += [np.unravel_index(next_state,env.shape)]\n",
    "        value += reward\n",
    "        action += [next_action]\n",
    "        if done:\n",
    "            return path, action, value\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt_path,action,value = get_optimal_path(Q,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib auto\n",
    "world = deepcopy(env.winds)\n",
    "t = 0\n",
    "for i in opt_path[:-1]:\n",
    "    world[i] = 6\n",
    "#     world[i] += action[t]\n",
    "    t+=1\n",
    "plt.imshow(world)\n",
    "# print value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
