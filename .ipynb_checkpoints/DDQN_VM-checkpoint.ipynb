{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from env_current import *\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed = 123):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "        \n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "    exploration_rate = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Exploration\", exploration_rate)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, exploration_rate]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNet(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, batch_size, save_path):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.inputs, self.q_values, self.a_predict = self.build_net()\n",
    "        self.net_params = tf.trainable_variables()\n",
    "        \n",
    "        self.target_inputs, self.target_q_values, self.target_a_predict = self.build_net()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params):]\n",
    "        \n",
    "        self.update_target_net_params = [self.target_net_params[i]\n",
    "                                         .assign(tf.multiply(self.tau, self.net_params[i])\n",
    "                                                 + tf.multiply((1.-self.tau), self.target_net_params[i]) ) \n",
    "                                         for i in range(len(self.target_net_params))]\n",
    "        \n",
    "        self.true_q_value = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "        \n",
    "        gather_indices = tf.range(MINIBATCH_SIZE) * tf.shape(self.q_values)[1] + tf.reshape(self.action, [-1])\n",
    "        self.action_correlated_q = tf.gather(tf.reshape(self.q_values,[-1]), gather_indices)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(tf.reshape(self.true_q_value, [-1]), self.action_correlated_q)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.last_num_epi = -1\n",
    "        \n",
    "    def build_net(self):\n",
    "        s_inputs = tf.placeholder(shape = [None, self.state_dim], dtype = tf.float32)\n",
    "        W1 = tf.Variable(tf.random_uniform([self.state_dim, 400], 0, 0.1))\n",
    "        B1 = tf.Variable(tf.zeros([400]))\n",
    "        L1 = tf.add(tf.matmul(s_inputs, W1), B1)\n",
    "        L1 = tf.layers.batch_normalization(L1)\n",
    "        L1 = tf.nn.relu(L1)\n",
    "        W2 = tf.Variable(tf.random_uniform([400, 300], 0, 0.1))\n",
    "        B2 = tf.Variable(tf.zeros([300]))\n",
    "        L2 = tf.add(tf.matmul(L1, W2), B2)\n",
    "        L2 = tf.layers.batch_normalization(L2)\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        W3 = tf.Variable(tf.random_uniform([300, self.action_dim], 0, 0.01))\n",
    "#         B3 = tf.Variable(tf.random_uniform([self.action_dim], -0.003, 0.003))\n",
    "#         q_values = tf.add(tf.matmul(L2, W3), B3)\n",
    "        q_values = tf.matmul(L2, W3)  \n",
    "        a_predict = tf.argmax(q_values,1)\n",
    "        \n",
    "        regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "        tf.contrib.layers.apply_regularization(regularizer,[W1, B1, W2, B2, W3])\n",
    "        return s_inputs, q_values, a_predict\n",
    "    \n",
    "    def train(self, states, action, true_q, num_epi):\n",
    "        if num_epi%20 == 0 and num_epi!=self.last_num_epi:\n",
    "            self.saver.save(self.sess, self.save_path)\n",
    "            print \"DDQN Saved\"\n",
    "            self.last_num_epi = num_epi\n",
    "            \n",
    "        return self.sess.run([self.q_values, self.optimizer], \n",
    "                             feed_dict={self.inputs: states, self.true_q_value: true_q, self.action: action})\n",
    "    \n",
    "    def predict_q(self, states):\n",
    "        return self.sess.run(self.q_values, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predict_a(self, states):\n",
    "        return self.sess.run(self.a_predict, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predect_target(self, states):\n",
    "        return self.sess.run(self.target_q_values, feed_dict={self.target_inputs: states})\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.sess.run(self.update_target_net_params)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, env, qnet):\n",
    "    \n",
    "    global EXPLORATION_RATE\n",
    "  \n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    \n",
    "    qnet.update_target()\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    for num_epi in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "        s = [list(np.unravel_index(s, env.shape))]\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "\n",
    "            a = np.argmax(qnet.predict_q(np.reshape(s, (1, qnet.state_dim))))\n",
    "    \n",
    "            if np.random.rand(1) < EXPLORATION_RATE:\n",
    "                s2, r, terminal, info = env.step(np.random.randint(0,qnet.action_dim))\n",
    "            else:\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "            \n",
    "            s2 = list(np.unravel_index(s2, env.shape))\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (qnet.state_dim,)), np.reshape(a, (1,)), r,\n",
    "                              terminal, np.reshape(s2, (qnet.state_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = qnet.predect_target(s2_batch)\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * np.amax(target_q[k]))\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = qnet.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)), num_epi)\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "                # Update target networks\n",
    "                qnet.update_target()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j == MAX_EPISODE_LEN-1:\n",
    "                \n",
    "                if EXPLORATION_RATE > 0.02 and terminal:\n",
    "                    EXPLORATION_RATE = EXPLORATION_RATE*0.92\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j),\n",
    "                    summary_vars[2]: EXPLORATION_RATE\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, num_epi)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} | Exploration: {:.6f} '.format(int(ep_reward), \\\n",
    "                        num_epi, (ep_ave_max_q / float(j)), EXPLORATION_RATE))\n",
    "                \n",
    "                f = open(\"stats.txt\", \"ab\")\n",
    "                f.write(\"| Reward: \" + str(int(ep_reward)) \n",
    "                        +\" | Episode: \" + str(num_epi) \n",
    "                        + \" | Qmax: \" + str(ep_ave_max_q / float(j)) \n",
    "                        + \" | Exploration: \" + str(EXPLORATION_RATE) + \"\\n\")\n",
    "                f.close()\n",
    "                \n",
    "                break\n",
    "                \n",
    "        if num_epi%1 == 0:\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            world = np.zeros(env.shape)\n",
    "            for state in range(env.nS):\n",
    "                state = np.unravel_index(state, env.shape)\n",
    "                action = qnet.predict_q(np.reshape(state, (1,state_dim)))\n",
    "                action = np.argmax(action)\n",
    "                state_list.append(state)\n",
    "                action_list.append(action)\n",
    "                \n",
    "#             print np.reshape(action_list, env.shape)\n",
    "                \n",
    "            f = open(\"action.txt\",\"ab\")\n",
    "            np.savetxt(f, np.reshape(action_list, env.shape), fmt=\"%i\")\n",
    "            f.write(\"---------------------------\\n\")\n",
    "            f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0015\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 10**6\n",
    "MINIBATCH_SIZE = 64\n",
    "RANDOM_SEED = 272\n",
    "MAX_EPISODES = 50000\n",
    "MAX_EPISODE_LEN = 1500\n",
    "SUMMARY_DIR = './results/tf_ddqn'\n",
    "EXPLORATION_RATE = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Saved\n",
      "| Reward: -1500 | Episode: 0 | Qmax: 75.4769 | Exploration: 0.650000 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = CurrentWorld()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    env.seed(RANDOM_SEED)\n",
    "    \n",
    "    state_dim = 2\n",
    "    action_dim = 4\n",
    "    \n",
    "    Qnet = QNet(sess, state_dim, action_dim, LEARNING_RATE, TAU, MINIBATCH_SIZE, \"./saved_model/ddqn.ckpt\")\n",
    "    \n",
    "    train(sess, env, Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "world = np.zeros(env.shape)\n",
    "a_list = []\n",
    "s_list = []\n",
    "for s in range(env.nS):\n",
    "    a_list += [np.argmax(P[s])]\n",
    "    s_list += [np.unravel_index(s,env.shape)]\n",
    "for s,a in zip(s_list,a_list):\n",
    "    world[s] = a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib auto\n",
    "plt.imshow(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_optimal_path(Q,env):\n",
    "    env.reset()\n",
    "    start_state = env.start_state\n",
    "    terminal_state = env.terminal_state\n",
    "    state = np.ravel_multi_index(start_state,env.shape)\n",
    "    path = [start_state]\n",
    "    value = 0\n",
    "    action = []\n",
    "    while 1:\n",
    "        next_action = np.argmax(Q[state])\n",
    "        next_state,reward,done,_ = env.step(next_action)\n",
    "        path += [np.unravel_index(next_state,env.shape)]\n",
    "        value += reward\n",
    "        action += [next_action]\n",
    "        if done:\n",
    "            return path, action, value\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt_path,action,value = get_optimal_path(Q,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib auto\n",
    "world = deepcopy(env.winds)\n",
    "t = 0\n",
    "for i in opt_path[:-1]:\n",
    "    world[i] = 6\n",
    "#     world[i] += action[t]\n",
    "    t+=1\n",
    "plt.imshow(world)\n",
    "# print value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
